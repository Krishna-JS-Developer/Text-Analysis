{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgU9Vt9A-1ui",
        "outputId": "f35d2d30-629c-4295-9e6f-6f9496568219"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import urllib3\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "url=pd.read_excel(\"Input.xlsx\")"
      ],
      "metadata": {
        "id": "gmH5T9oW_CBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_files = [\"StopWords_Auditor.txt\",\"StopWords_Currencies.txt\",\"StopWords_DatesandNumbers.txt\",\"StopWords_Generic.txt\",\"StopWords_GenericLong.txt\",\"StopWords_Geographic.txt\",\"StopWords_Names.txt\"]\n",
        "custom_stopwords = set()\n",
        "for filename in stopwords_files:\n",
        "    with open(filename,'r',encoding='latin-1') as file:\n",
        "        custom_stopwords.update(file.read().splitlines())"
      ],
      "metadata": {
        "id": "6WR52q5E_Evk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_list = []\n",
        "for _, row in url.iterrows():\n",
        "    url_id = row[\"URL_ID\"]\n",
        "    current_url = row[\"URL\"]\n",
        "    http = urllib3.PoolManager()\n",
        "    response = http.request(\"GET\",current_url)\n",
        "    if response.status == 200:\n",
        "      html_content = response.data.decode('latin-1')\n",
        "      soup = BeautifulSoup(html_content, 'html.parser')\n",
        "      paragraph_tags = soup.find_all(['p','h1'])\n",
        "      paragraph_text = [tag.get_text() for tag in paragraph_tags]\n",
        "      paragraph_text = paragraph_text[16:-3]\n",
        "      paragraph_text = '\\n'.join(paragraph_text)\n",
        "      sentences = sent_tokenize(paragraph_text)\n",
        "      paragraph_text = paragraph_text.replace('?', '').replace('!', '').replace(',', '').replace('.', '')\n",
        "      #Positive and Negative words\n",
        "      words = word_tokenize(paragraph_text)\n",
        "      #Saving filtered words with url_id as name\n",
        "      filtered_words = [word for word in words if word.lower() not in custom_stopwords]\n",
        "      file = open(f\"{url_id}.txt\", \"w\", encoding='utf-8')\n",
        "      file.write(paragraph_text)\n",
        "      file.close()\n",
        "      positive_words = set()\n",
        "      negative_words = set()\n",
        "      with open('positive-words.txt', 'r', encoding='latin-1') as positive_file:\n",
        "          positive_words.update(positive_file.read().splitlines())\n",
        "      with open('negative-words.txt', 'r', encoding='latin-1') as negative_file:\n",
        "          negative_words.update(negative_file.read().splitlines())\n",
        "      positive_count = 0\n",
        "      negative_count = 0\n",
        "      for word in filtered_words:\n",
        "          if word.lower() in positive_words:\n",
        "              positive_count += 1\n",
        "          elif word.lower() in negative_words:\n",
        "              negative_count += 1\n",
        "      #Polarity Score\n",
        "      polarity_score=(positive_count - negative_count)/ ((positive_count + negative_count) + 0.000001)\n",
        "      #Word count\n",
        "      total_word_count=len(filtered_words)\n",
        "      #Subjective score\n",
        "      subjective_score=(positive_count + negative_count)/ ((len(filtered_words)) + 0.000001)\n",
        "      #Average Sentence Length\n",
        "      sentence_length=len(sentences)\n",
        "      avg_sentence_length=len(words)/sentence_length\n",
        "      #Syllable count and Complex word count\n",
        "      syllable_count=0\n",
        "      complex_words=0\n",
        "      for word in filtered_words:\n",
        "        count=0\n",
        "        for i in range(len(word)):\n",
        "          if(word[i]=='a' or word[i]=='e' or word[i] =='i' or word[i] == 'o' or word[i] == 'u'):\n",
        "                count+=1\n",
        "          if(i==len(word)-2 and (word[i]=='e' and word[i+1]=='d')):\n",
        "            count-=1;\n",
        "          if(i==len(word)-2 and (word[i]=='e' and word[i+1]=='s')):\n",
        "            count-=1;\n",
        "        syllable_count+=count\n",
        "        if(count>2):\n",
        "            complex_words+=1\n",
        "      #Syllable per word\n",
        "      syllable_per_word=syllable_count/total_word_count\n",
        "      #Percentage of complex words\n",
        "      percent_complex_words=complex_words/total_word_count\n",
        "      #Fog index\n",
        "      fog_index=0.4 * (avg_sentence_length + percent_complex_words)\n",
        "      #Average number of words\n",
        "      avg_number_words=len(words)/len(sentences)\n",
        "      #Average number of words per sentence\n",
        "      avg_number_words_sentence=len(words)/len(sentences)\n",
        "      #Personal Pronoun\n",
        "      personal_pronoun_count=0\n",
        "      personal_pronouns = ['I','we','We','my','My','ours','Ours','us' ]\n",
        "      for word in words:\n",
        "        if word in personal_pronouns:\n",
        "          personal_pronoun_count+=1\n",
        "      #Average word length\n",
        "      total_word_length = sum(len(word) for word in words)\n",
        "      avg_word_length = total_word_length / len(words)\n",
        "      result_dict = {\n",
        "            'URL_ID': url_id,\n",
        "            'URL': current_url,\n",
        "            'POSITIVE SCORE': positive_count,\n",
        "            'NEGATIVE SCORE': negative_count,\n",
        "            'POLARITY SCORE': polarity_score,\n",
        "            'SUBJECTIVITY SCORE': subjective_score,\n",
        "            'AVG SENTENCE LENGTH': avg_sentence_length,\n",
        "            'PERCENTAGE OF COMPLEX WORDS': percent_complex_words,\n",
        "            'FOG INDEX': fog_index,\n",
        "            'AVG NUMBER OF WORDS PER SENTENCE':avg_number_words_sentence,\n",
        "            'COMPLEX WORD COUNT': complex_words,\n",
        "            'WORD COUNT': total_word_count,\n",
        "            'SYLLABLE PER WORD':syllable_per_word,\n",
        "            'PERSONAL PRONOUNS': personal_pronoun_count,\n",
        "            'AVG WORD LENGTH': avg_word_length\n",
        "\n",
        "        }\n",
        "      results_list.append(result_dict)\n",
        "    else:\n",
        "        print(f\"Failed to fetch data from {current_url}. Status code: {response.status}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xA8HNBNQ_QjI",
        "outputId": "3e0d82da-25ea-47ab-c685-7e6845f523e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch data from https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/. Status code: 404\n",
            "Failed to fetch data from https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/. Status code: 404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = pd.DataFrame(results_list)\n",
        "results_df.to_excel(\"Output Data Structure.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "jlM3gTYs_nNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZUBmsgeDAVDJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}